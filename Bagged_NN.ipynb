{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzqYSR2XOoCiO/3Hxuxrq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manav9547/AI-ChatBot/blob/main/Bagged_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFtFmvR4zuM",
        "outputId": "b69c1ae0-1f5e-4fb1-a263-16b83d7ee675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Fitting preprocessor...\n",
            "Processed Feature Count: 35\n",
            "\n",
            "Training Polynomial SVM...\n",
            "SVM Val AUC: 0.7460\n",
            "\n",
            "Training Bagged Neural Networks (5 estimators)...\n",
            "Bagged NN Val AUC: 0.7468\n",
            "Ensemble Val AUC: 0.7509\n",
            "\n",
            "Generating test predictions...\n",
            "Saved: submission_bagging_poly.csv\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Config\n",
        "# ------------------------------------------------------------------------------\n",
        "TRAIN_PATH = \"train_updated.csv\"\n",
        "TEST_PATH  = \"test_updated.csv\"\n",
        "SAMPLE_PATH = \"sample_submission_updated.csv\"\n",
        "TARGET = \"RiskFlag\"\n",
        "ID_COL = \"ProfileID\"\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Load Data\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Advanced Feature Engineering\n",
        "# ------------------------------------------------------------------------------\n",
        "# Based on analysis, these cols have skewed distributions (power law).\n",
        "# Log-transforming them helps the Neural Network significantly.\n",
        "log_cols = ['AnnualEarnings', 'RequestedSum', 'WorkDuration', 'TrustMetric']\n",
        "\n",
        "# These are the top predictors. We will generate interaction terms for them.\n",
        "poly_cols = ['ApplicantYears', 'OfferRate', 'DebtFactor']\n",
        "\n",
        "# Categorical columns for OneHot\n",
        "cat_cols = ['QualificationLevel', 'WorkCategory', 'RelationshipStatus',\n",
        "            'OwnsProperty', 'FamilyObligation', 'FundUseCase', 'JointApplicant']\n",
        "\n",
        "# Define transformations\n",
        "def log_transform(x):\n",
        "    return np.log1p(np.maximum(x, 0))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Preprocessing Pipelines\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Pipeline A: Log Transform + Scaling (For skewed data)\n",
        "log_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('log', FunctionTransformer(log_transform)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline B: Polynomial Features (For top predictors)\n",
        "# Degree 2 generates interactions: Age^2, Age*Rate, Rate^2, etc.\n",
        "poly_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline C: Categorical\n",
        "cat_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combine them\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('log', log_transformer, log_cols),\n",
        "        ('poly', poly_transformer, poly_cols),\n",
        "        ('cat', cat_transformer, cat_cols)\n",
        "    ],\n",
        "    remainder='drop' # Drop other columns to reduce noise\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Prepare Train/Validation Sets\n",
        "# ------------------------------------------------------------------------------\n",
        "X = train.drop(columns=[TARGET, ID_COL])\n",
        "y = train[TARGET]\n",
        "X_test = test.drop(columns=[ID_COL])\n",
        "\n",
        "# Split for internal validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Fitting preprocessor...\")\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_val_proc = preprocessor.transform(X_val)\n",
        "X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Processed Feature Count: {X_train_proc.shape[1]}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Model A: Polynomial Linear SVM\n",
        "# ------------------------------------------------------------------------------\n",
        "# Since we added Polynomial features in the pipeline, a LinearSVC here\n",
        "# effectively acts like a Polynomial Kernel SVM but is much faster.\n",
        "print(\"\\nTraining Polynomial SVM...\")\n",
        "\n",
        "svm_base = LinearSVC(class_weight='balanced', random_state=RANDOM_STATE, max_iter=3000, dual=False)\n",
        "svm_calibrated = CalibratedClassifierCV(estimator=svm_base, method='isotonic', cv=3)\n",
        "\n",
        "svm_calibrated.fit(X_train_proc, y_train)\n",
        "\n",
        "svm_probs_val = svm_calibrated.predict_proba(X_val_proc)[:, 1]\n",
        "print(f\"SVM Val AUC: {roc_auc_score(y_val, svm_probs_val):.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Model B: Bagged Neural Networks (Ensemble)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Instead of 1 big NN, we train 5 smaller ones on subsets.\n",
        "# This reduces variance and typically improves generalization.\n",
        "print(\"\\nTraining Bagged Neural Networks (5 estimators)...\")\n",
        "\n",
        "nn_base = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    alpha=0.001,\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=200,\n",
        "    early_stopping=True,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# BaggingClassifier trains 'n_estimators' models on random subsets of data\n",
        "bagged_nn = BaggingClassifier(\n",
        "    estimator=nn_base,\n",
        "    n_estimators=5,\n",
        "    max_samples=0.7,  # Use 70% of data for each model\n",
        "    max_features=1.0, # Use all features\n",
        "    n_jobs=1,         # Set to -1 if running locally for parallelism\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "bagged_nn.fit(X_train_proc, y_train)\n",
        "\n",
        "nn_probs_val = bagged_nn.predict_proba(X_val_proc)[:, 1]\n",
        "print(f\"Bagged NN Val AUC: {roc_auc_score(y_val, nn_probs_val):.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Blending & Submission\n",
        "# ------------------------------------------------------------------------------\n",
        "# Weighted average: Give more weight to NN if it performs better\n",
        "ensemble_probs_val = (0.4 * svm_probs_val) + (0.6 * nn_probs_val)\n",
        "print(f\"Ensemble Val AUC: {roc_auc_score(y_val, ensemble_probs_val):.4f}\")\n",
        "\n",
        "# Predict on Test Set\n",
        "print(\"\\nGenerating test predictions...\")\n",
        "svm_test_probs = svm_calibrated.predict_proba(X_test_proc)[:, 1]\n",
        "nn_test_probs = bagged_nn.predict_proba(X_test_proc)[:, 1]\n",
        "\n",
        "ensemble_test_probs = (0.4 * svm_test_probs) + (0.6 * nn_test_probs)\n",
        "\n",
        "# Create Submission\n",
        "submission = pd.DataFrame({\n",
        "    ID_COL: test[ID_COL],\n",
        "    TARGET: ensemble_test_probs\n",
        "})\n",
        "\n",
        "# Reorder to match sample submission if needed\n",
        "sample_ids = sample[ID_COL].values\n",
        "submission = submission.set_index(ID_COL).reindex(sample_ids).reset_index()\n",
        "\n",
        "submission.to_csv(\"submission_bagging_poly.csv\", index=False)\n",
        "print(\"Saved: submission_bagging_poly.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "IN = \"submission_bagging_poly.csv\" # or your model output\n",
        "OUT = \"submission_binary_bagging.csv\"\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "predcol = df.columns[1]\n",
        "\n",
        "# ensure numeric\n",
        "df[predcol] = pd.to_numeric(df[predcol], errors=\"coerce\").fillna(0.5)\n",
        "\n",
        "# choose threshold (0.5 default). You can tune threshold on validation set.\n",
        "threshold = 0.5\n",
        "df[predcol] = (df[predcol] >= threshold).astype(int)\n",
        "\n",
        "df.to_csv(OUT, index=False)\n",
        "print(\"Saved binary submission to:\", OUT)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwwMaYSPkCR",
        "outputId": "615f6e5c-a1b4-4419-dba4-483043f62dc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved binary submission to: submission_binary_bagging.csv\n",
            "    ProfileID  RiskFlag\n",
            "0  CKV34LU7V7         0\n",
            "1  62KTYNH93J         0\n",
            "2  JGFUSOIUH7         0\n",
            "3  4538THBHOX         0\n",
            "4  DXLNA06JHR         0\n"
          ]
        }
      ]
    }
  ]
}