{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn26XVDLd6lnQO9uvbWsiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manav9547/AI-ChatBot/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFtFmvR4zuM",
        "outputId": "1c7de726-acae-4e62-9bd8-64f7db6ee453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Train shape: (204277, 18) Test shape: (51070, 17) Sample shape: (51070, 2)\n",
            "Numeric cols: ['ApplicantYears', 'AnnualEarnings', 'RequestedSum', 'TrustMetric', 'WorkDuration', 'ActiveAccounts', 'OfferRate', 'RepayPeriod', 'DebtFactor']\n",
            "Categorical cols: ['QualificationLevel', 'WorkCategory', 'RelationshipStatus', 'OwnsProperty', 'FamilyObligation', 'FundUseCase', 'JointApplicant']\n",
            "Fitting preprocessor...\n",
            "Transforming datasets...\n",
            "Transformed shapes (sparse): (163421, 31) (40856, 31) (51070, 31)\n",
            "\n",
            "Training LinearSVC + CalibratedClassifierCV...\n",
            "SVM-like Validation AUC: 0.7463\n",
            "SVM-like Validation ACC: 0.8848\n",
            "\n",
            "Requested SVD components: 200\n",
            "Number of features after preprocessing: 31\n",
            "Using n_components for SVD: 30\n",
            "\n",
            "Reducing dimensionality with TruncatedSVD (n_components=30)...\n",
            "Reduced shapes (dense): (163421, 30) (40856, 30) (51070, 30)\n",
            "\n",
            "Training MLPClassifier on reduced features...\n",
            "MLP Validation AUC: 0.7493\n",
            "MLP Validation ACC: 0.8855\n",
            "\n",
            "Predicting on test set and writing submission files...\n",
            "Saved files: svm_submission.csv, nn_submission.csv, ensemble_submission.csv\n",
            "\n",
            "Ensemble Validation AUC: 0.7516\n",
            "Ensemble Validation ACC: 0.8855\n",
            "\n",
            "Done. If you hit MemoryError or long runtimes, try lowering DESIRED_SVD_COMPONENTS (e.g. 50-100) or train on a subset.\n"
          ]
        }
      ],
      "source": [
        "# full_pipeline_kaggle_fixed_svd.py\n",
        "# Robust pipeline (handles sklearn version differences and ensures SVD n_components <= n_features-1).\n",
        "# Run in Colab / Kaggle / Jupyter.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import inspect\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "TRAIN_PATH = \"train_updated.csv\"\n",
        "TEST_PATH  = \"test_updated.csv\"\n",
        "SAMPLE_PATH = \"sample_submission_updated.csv\"\n",
        "\n",
        "TARGET = \"RiskFlag\"\n",
        "IDCOL  = \"ProfileID\"\n",
        "\n",
        "# desired SVD components (will be clamped to valid range automatically)\n",
        "DESIRED_SVD_COMPONENTS = 200\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# -------------------------\n",
        "# Load\n",
        "# -------------------------\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "print(\"Train shape:\", train.shape, \"Test shape:\", test.shape, \"Sample shape:\", sample.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Split\n",
        "# -------------------------\n",
        "X = train.drop(columns=[TARGET])\n",
        "y = train[TARGET].astype(int)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "all_features = [c for c in X_train.columns if c != IDCOL]\n",
        "numeric_cols = X_train[all_features].select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = [c for c in all_features if c not in numeric_cols]\n",
        "\n",
        "print(\"Numeric cols:\", numeric_cols)\n",
        "print(\"Categorical cols:\", categorical_cols)\n",
        "\n",
        "# -------------------------\n",
        "# Preprocess\n",
        "# -------------------------\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))   # keep sparse\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipeline, numeric_cols),\n",
        "        (\"cat\", categorical_pipeline, categorical_cols)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "print(\"Fitting preprocessor...\")\n",
        "preprocessor.fit(X_train[all_features])\n",
        "\n",
        "print(\"Transforming datasets...\")\n",
        "X_train_trans = preprocessor.transform(X_train[all_features])\n",
        "X_val_trans   = preprocessor.transform(X_val[all_features])\n",
        "X_test_trans  = preprocessor.transform(test[all_features])\n",
        "\n",
        "# Ensure sparse CSR\n",
        "if not sparse.issparse(X_train_trans):\n",
        "    X_train_s = sparse.csr_matrix(X_train_trans)\n",
        "    X_val_s   = sparse.csr_matrix(X_val_trans)\n",
        "    X_test_s  = sparse.csr_matrix(X_test_trans)\n",
        "else:\n",
        "    X_train_s = sparse.csr_matrix(X_train_trans)\n",
        "    X_val_s   = sparse.csr_matrix(X_val_trans)\n",
        "    X_test_s  = sparse.csr_matrix(X_test_trans)\n",
        "\n",
        "print(\"Transformed shapes (sparse):\", X_train_s.shape, X_val_s.shape, X_test_s.shape)\n",
        "\n",
        "# -------------------------\n",
        "# LinearSVC + CalibratedClassifierCV (compatibility handling)\n",
        "# -------------------------\n",
        "print(\"\\nTraining LinearSVC + CalibratedClassifierCV...\")\n",
        "\n",
        "base_clf = LinearSVC(max_iter=4000, random_state=RANDOM_STATE)\n",
        "\n",
        "# instantiate CalibratedClassifierCV using estimator= or base_estimator= depending on sklearn\n",
        "calib_kwargs = {}\n",
        "calib_sig = inspect.signature(CalibratedClassifierCV.__init__)\n",
        "if \"estimator\" in calib_sig.parameters:\n",
        "    calib_kwargs[\"estimator\"] = base_clf\n",
        "elif \"base_estimator\" in calib_sig.parameters:\n",
        "    calib_kwargs[\"base_estimator\"] = base_clf\n",
        "else:\n",
        "    calib_kwargs[\"estimator\"] = base_clf\n",
        "\n",
        "calib_kwargs[\"cv\"] = 3\n",
        "calib_kwargs[\"method\"] = \"sigmoid\"\n",
        "\n",
        "try:\n",
        "    svm_clf = CalibratedClassifierCV(**calib_kwargs)\n",
        "    svm_clf.fit(X_train_s, y_train)\n",
        "\n",
        "    svm_val_probs = svm_clf.predict_proba(X_val_s)[:, 1]\n",
        "    svm_val_pred = (svm_val_probs >= 0.5).astype(int)\n",
        "\n",
        "    print(\"SVM-like Validation AUC: {:.4f}\".format(roc_auc_score(y_val, svm_val_probs)))\n",
        "    print(\"SVM-like Validation ACC: {:.4f}\".format(accuracy_score(y_val, svm_val_pred)))\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Failed to train calibrated LinearSVC: {}\".format(e))\n",
        "\n",
        "# -------------------------\n",
        "# TruncatedSVD -> MLP pipeline\n",
        "# Ensure n_components is valid (<= n_features). Clamp automatically.\n",
        "# -------------------------\n",
        "n_features = X_train_s.shape[1]\n",
        "max_valid = max(1, n_features - 1)  # keep at least 1 component\n",
        "n_components = min(DESIRED_SVD_COMPONENTS, max_valid)\n",
        "\n",
        "print(\"\\nRequested SVD components:\", DESIRED_SVD_COMPONENTS)\n",
        "print(\"Number of features after preprocessing:\", n_features)\n",
        "print(\"Using n_components for SVD:\", n_components)\n",
        "\n",
        "print(\"\\nReducing dimensionality with TruncatedSVD (n_components={})...\".format(n_components))\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\n",
        "svd.fit(X_train_s)\n",
        "\n",
        "X_train_reduced = svd.transform(X_train_s)\n",
        "X_val_reduced   = svd.transform(X_val_s)\n",
        "X_test_reduced  = svd.transform(X_test_s)\n",
        "\n",
        "print(\"Reduced shapes (dense):\", X_train_reduced.shape, X_val_reduced.shape, X_test_reduced.shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_nn = scaler.fit_transform(X_train_reduced)\n",
        "X_val_nn   = scaler.transform(X_val_reduced)\n",
        "X_test_nn  = scaler.transform(X_test_reduced)\n",
        "\n",
        "print(\"\\nTraining MLPClassifier on reduced features...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,64),\n",
        "                    alpha=0.001,\n",
        "                    max_iter=300,\n",
        "                    early_stopping=True,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    verbose=False)\n",
        "\n",
        "mlp.fit(X_train_nn, y_train)\n",
        "\n",
        "mlp_val_probs = mlp.predict_proba(X_val_nn)[:, 1]\n",
        "mlp_val_pred = (mlp_val_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"MLP Validation AUC: {:.4f}\".format(roc_auc_score(y_val, mlp_val_probs)))\n",
        "print(\"MLP Validation ACC: {:.4f}\".format(accuracy_score(y_val, mlp_val_pred)))\n",
        "\n",
        "# -------------------------\n",
        "# Test predictions & submission\n",
        "# -------------------------\n",
        "print(\"\\nPredicting on test set and writing submission files...\")\n",
        "svm_test_probs = svm_clf.predict_proba(X_test_s)[:, 1]\n",
        "mlp_test_probs = mlp.predict_proba(X_test_nn)[:, 1]\n",
        "ensemble_probs = (svm_test_probs + mlp_test_probs) / 2.0\n",
        "\n",
        "svm_sub = pd.DataFrame({IDCOL: test[IDCOL], TARGET: svm_test_probs})\n",
        "nn_sub  = pd.DataFrame({IDCOL: test[IDCOL], TARGET: mlp_test_probs})\n",
        "ens_sub = pd.DataFrame({IDCOL: test[IDCOL], TARGET: ensemble_probs})\n",
        "\n",
        "# reorder to match sample submission if possible\n",
        "try:\n",
        "    svm_sub = svm_sub.set_index(IDCOL).loc[sample[IDCOL]].reset_index()\n",
        "    nn_sub  = nn_sub.set_index(IDCOL).loc[sample[IDCOL]].reset_index()\n",
        "    ens_sub = ens_sub.set_index(IDCOL).loc[sample[IDCOL]].reset_index()\n",
        "except Exception:\n",
        "    print(\"Warning: could not reindex to sample file order â€” writing in test order instead.\")\n",
        "\n",
        "svm_sub.to_csv(\"svm_submission.csv\", index=False)\n",
        "nn_sub.to_csv(\"nn_submission.csv\", index=False)\n",
        "ens_sub.to_csv(\"ensemble_submission.csv\", index=False)\n",
        "\n",
        "print(\"Saved files: svm_submission.csv, nn_submission.csv, ensemble_submission.csv\")\n",
        "\n",
        "# -------------------------\n",
        "# Ensemble validation metrics\n",
        "# -------------------------\n",
        "try:\n",
        "    ensemble_val_probs = (svm_val_probs + mlp_val_probs) / 2.0\n",
        "    ensemble_val_pred = (ensemble_val_probs >= 0.5).astype(int)\n",
        "    print(\"\\nEnsemble Validation AUC: {:.4f}\".format(roc_auc_score(y_val, ensemble_val_probs)))\n",
        "    print(\"Ensemble Validation ACC: {:.4f}\".format(accuracy_score(y_val, ensemble_val_pred)))\n",
        "except Exception:\n",
        "    print(\"Could not compute ensemble validation metrics (one model may have failed).\")\n",
        "\n",
        "print(\"\\nDone. If you hit MemoryError or long runtimes, try lowering DESIRED_SVD_COMPONENTS (e.g. 50-100) or train on a subset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "IN = \"svm_submission.csv\"  # or your model output\n",
        "OUT = \"svm_submission_binary.csv\"\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "predcol = df.columns[1]\n",
        "\n",
        "# ensure numeric\n",
        "df[predcol] = pd.to_numeric(df[predcol], errors=\"coerce\").fillna(0.5)\n",
        "\n",
        "# choose threshold (0.5 default). You can tune threshold on validation set.\n",
        "threshold = 0.5\n",
        "df[predcol] = (df[predcol] >= threshold).astype(int)\n",
        "\n",
        "df.to_csv(OUT, index=False)\n",
        "print(\"Saved binary submission to:\", OUT)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwwMaYSPkCR",
        "outputId": "1bf6c6ef-8e28-46fa-df6b-31160599a2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved binary submission to: svm_submission_binary.csv\n",
            "    ProfileID  RiskFlag\n",
            "0  CKV34LU7V7         0\n",
            "1  62KTYNH93J         0\n",
            "2  JGFUSOIUH7         0\n",
            "3  4538THBHOX         0\n",
            "4  DXLNA06JHR         0\n"
          ]
        }
      ]
    }
  ]
}